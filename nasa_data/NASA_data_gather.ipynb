{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c541a020",
   "metadata": {
    "id": "c541a020"
   },
   "source": [
    "# Pulling NETCDF files from NASA, Aggregating files, Converting to .CSV\n",
    "### Objective:\n",
    "- There is a specific way to pull down data from NASA\n",
    "- netCDF files are large size and can take much more resourced to compute for data analysis\n",
    "- Convert netCDF to CSV format to reduce the size of data\n",
    "\n",
    "\n",
    "### STEPS: \n",
    "* Ensure that you have the correct credentials to access the data\n",
    "* Setup your environment to work with wget\n",
    "* Use wget to pull down daily files from a subset link provided by NASA \n",
    "* Do some preliminary filtering \n",
    "* Concatenate the data files\n",
    "* final output: master csv file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd3e4835",
   "metadata": {},
   "source": [
    "### Ensure that you have the following completed: \n",
    "* You have registered for an EarthData Login Profile: https://wiki.earthdata.nasa.gov/display/EL/How+To+Register+For+an+EarthData+Login+Profile\n",
    "* You have authorized \"NASA GESDISC DATA ARCHIVE\": https://disc.gsfc.nasa.gov/earthdata-login\n",
    "* Generate the prerequisite files needed for the wget tool: https://disc.gsfc.nasa.gov/information/howto?title=How%20to%20Generate%20Earthdata%20Prerequisite%20Files\n",
    "* Download the wget tool \n",
    "    * This is a little tricky, I was running into authorization issues on the NASA site using the most recent version of wget. I found successs using version 1.19.2\n",
    "    * You can download from: https://eternallybored.org/misc/wget/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f39f5826",
   "metadata": {},
   "source": [
    "## Subset the data as needed\n",
    "\n",
    "At the moment, the dataset that we are using can be found here: https://disc.gsfc.nasa.gov/api/jobs/results/6480ba7f9c692c7cd8c4a794\n",
    "\n",
    "A handy subsetting tool is available\n",
    "\n",
    " ![Alt text](subset_image.jpg)\n",
    "\n",
    " From there, you can subset on a specific region, date range, etc. \n",
    "\n",
    " ![Alt text](subset_region.jpg)\n",
    "\n",
    " Once you are finished, you will be directed to this screen where you need to download the list of links provided. The list should be as long as the the number of days that you are subsetting from since each link directs to a data file for one day of readings.\n",
    "\n",
    " ![Alt text](download_links.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54093d0e",
   "metadata": {},
   "source": [
    "## Get the data with wget\n",
    "\n",
    "If you have correctly completed all the previous steps, this should work for you\n",
    "* Open a command prompt on your PC\n",
    "* Set your working directory to the location of your link list as well as the .dodsrc file you created (see the prequisite files mentioned above)\n",
    "* From the command line, run: \n",
    "\n",
    "wget --load-cookies <path of .urs_cookies file> --save-cookies <path of .urs_cookies file> --keep-session-cookies --user=< YOURUSERNAME > --ask-password -P <folder you want to save the data to> --content-disposition -i <link list .txt>\n",
    "\n",
    "For example:\n",
    "\n",
    "wget --load-cookies C:\\Users\\bucky\\ .urs_cookies --save-cookies C:\\Users\\bucky\\ .urs_cookies --keep-session-cookies --user=bbucky --ask-password -P data_2014/ --content-disposition -i 2014_subset_data_links.txt\n",
    "\n",
    "* it takes me roughly 10 seconds to download one file, for reference\n",
    "\n",
    "You should now have your netcdf files downloaded\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8c014d2",
   "metadata": {},
   "source": [
    "## Convert and concatenate multiple netCDF files to master CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a680156",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1652679167658,
     "user": {
      "displayName": "Kikipessa Doll",
      "userId": "16203075636725704864"
     },
     "user_tz": 420
    },
    "id": "9a680156"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import netCDF4 as nc\n",
    "\n",
    "# converting the datetime format\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab238c96",
   "metadata": {
    "id": "ab238c96"
   },
   "source": [
    "## Path to NETCDF files\n",
    "- Locate the downloaded netcdf files directory in pc directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fa1e6c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 214,
     "status": "error",
     "timestamp": 1652679503238,
     "user": {
      "displayName": "Kikipessa Doll",
      "userId": "16203075636725704864"
     },
     "user_tz": 420
    },
    "id": "1fa1e6c8",
    "outputId": "7e95debd-55c7-47b2-e114-2120a08c6304"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data\\\\data_netcdf\\\\oco2_LtCO2_140906_B11014Ar_230329213249s.SUB.nc4',\n",
       " 'data\\\\data_netcdf\\\\oco2_LtCO2_140907_B11014Ar_230329213320s.SUB.nc4',\n",
       " 'data\\\\data_netcdf\\\\oco2_LtCO2_140908_B11014Ar_230329213350s.SUB.nc4',\n",
       " 'data\\\\data_netcdf\\\\oco2_LtCO2_140909_B11014Ar_230329213426s.SUB.nc4',\n",
       " 'data\\\\data_netcdf\\\\oco2_LtCO2_140910_B11014Ar_230329213451s.SUB.nc4',\n",
       " 'data\\\\data_netcdf\\\\oco2_LtCO2_140911_B11014Ar_230329213526s.SUB.nc4',\n",
       " 'data\\\\data_netcdf\\\\oco2_LtCO2_140912_B11014Ar_230329213557s.SUB.nc4',\n",
       " 'data\\\\data_netcdf\\\\oco2_LtCO2_140913_B11014Ar_230329213633s.SUB.nc4',\n",
       " 'data\\\\data_netcdf\\\\oco2_LtCO2_140914_B11014Ar_230329213705s.SUB.nc4',\n",
       " 'data\\\\data_netcdf\\\\oco2_LtCO2_140915_B11014Ar_230329213733s.SUB.nc4']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_a= ('data\\data_netcdf')\n",
    "\n",
    "# Collect the paths of each individual files\n",
    "file_names= []\n",
    "\n",
    "for file in os.listdir(path_a):\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".nc4\"):\n",
    "        file_path = f\"{path_a}\\{file}\"\n",
    "      \n",
    "        # Store the path location of each individual files\n",
    "        file_names.append(file_path)\n",
    "        \n",
    "        \n",
    "# check first 10 files path\n",
    "file_names[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd2ad694",
   "metadata": {
    "id": "cd2ad694"
   },
   "source": [
    "# Check the total files in the DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de96547b",
   "metadata": {
    "id": "de96547b",
    "outputId": "2c929c0a-183e-447c-d3cb-2bdb0dba76ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TotalFiles:  777\n"
     ]
    }
   ],
   "source": [
    "countFiles=0\n",
    "\n",
    "for j in file_names:\n",
    "    if j.endswith(\".nc4\"):\n",
    "        countFiles+=1\n",
    "        #print(j)\n",
    "        \n",
    "print('\\nTotalFiles: ', countFiles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4a27bf7",
   "metadata": {
    "id": "e4a27bf7"
   },
   "source": [
    "# Example: \n",
    "### Opening a single file in netCDF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71c5c0f8",
   "metadata": {
    "id": "71c5c0f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sounding_id_idx',\n",
       " 'longitude',\n",
       " 'latitude',\n",
       " 'time',\n",
       " 'epoch_dimension',\n",
       " 'co2_profile_apriori',\n",
       " 'date',\n",
       " 'file_index',\n",
       " 'pressure_levels',\n",
       " 'pressure_weight',\n",
       " 'sensor_zenith_angle',\n",
       " 'solar_zenith_angle',\n",
       " 'vertex_latitude',\n",
       " 'vertex_longitude',\n",
       " 'xco2',\n",
       " 'xco2_apriori',\n",
       " 'xco2_averaging_kernel',\n",
       " 'xco2_qf_bitflag',\n",
       " 'xco2_quality_flag',\n",
       " 'xco2_uncertainty',\n",
       " 'sounding_id',\n",
       " 'levels',\n",
       " 'vertices']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xco2= nc.Dataset(file_names[9])\n",
    "list(df_xco2.variables.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f1aa9ba",
   "metadata": {
    "id": "8f1aa9ba"
   },
   "source": [
    "# DateTime format Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4133c955",
   "metadata": {
    "id": "4133c955"
   },
   "outputs": [],
   "source": [
    "# DATE time function\n",
    "def conv_date(d):\n",
    "    return datetime.strptime(str(d), '%Y%m%d%H%M%S%f')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7d135ca",
   "metadata": {
    "id": "d7d135ca"
   },
   "source": [
    "### NOTE:\n",
    "- Refine the ENTIRE dataframe by GOOD quality_flag->0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bfa8617",
   "metadata": {
    "id": "5bfa8617"
   },
   "outputs": [],
   "source": [
    "# FUNCTION to convert data\n",
    "\n",
    "def convHdf(path_file, n=0):\n",
    "\n",
    "    data= nc.Dataset(path_file)\n",
    "\n",
    "    #if empty, report and continue\n",
    "    if 'xco2' not in data.variables.keys():\n",
    "        print(path_file,\" is missing xco2 data\")\n",
    "        return \n",
    "\n",
    "    df_xco2= pd.DataFrame()\n",
    "    \n",
    "    #if we want to experiment with different variables\n",
    "    \"\"\"variables=['sounding_id_idx', 'longitude', 'latitude', 'time', 'epoch_dimension', 'co2_profile_apriori', 'date',\n",
    "                'file_index', 'pressure_levels', 'pressure_weight', 'sensor_zenith_angle', 'solar_zenith_angle', 'vertex_latitude',\n",
    "                  'vertex_longitude', 'xco2', 'xco2_apriori', 'xco2_averaging_kernel', 'xco2_qf_bitflag', 'xco2_quality_flag', 'xco2_uncertainty', 'sounding_id']\n",
    "\n",
    "    for i in variables:\n",
    "        print\n",
    "        try:\n",
    "            df_xco2[i]=data.variables[i][:]\n",
    "        except:\n",
    "            pass\"\"\"\n",
    "\n",
    "    df_xco2['Xco2']= data.variables['xco2'][:]\n",
    "    df_xco2['Latitude']= data.variables['latitude'][:]\n",
    "    df_xco2['Longitude']= data.variables['longitude'][:] \n",
    "    df_xco2['xco2_quality_flag']= data.variables['xco2_quality_flag'][:]\n",
    "    \n",
    "    # Date\n",
    "    df_xco2['DateTime']= data.variables['sounding_id'][:]\n",
    "    \n",
    "    #Convert soundingID to datetime format\n",
    "    df_xco2['DateTime']= df_xco2['DateTime'].apply(conv_date)\n",
    "    df_xco2['DateTime']= pd.to_datetime(df_xco2['DateTime'])\n",
    "    \n",
    "    # YEAR and month column\n",
    "    df_xco2['Year']= df_xco2['DateTime'].dt.year\n",
    "    df_xco2['Month']= df_xco2['DateTime'].dt.month\n",
    "    df_xco2['Day']= df_xco2['DateTime'].dt.day\n",
    "    \n",
    "    # Refine the ENTIRE dataframe by GOOD quality_flag->0\n",
    "    # NOTE: REDUCES the size of the file\n",
    "    df_xco2= df_xco2[df_xco2['xco2_quality_flag'] == 0]   \n",
    "    \n",
    "   \n",
    "    date= str(data.variables['sounding_id'][0])      \n",
    "    \n",
    "    return df_xco2\n",
    "    # create a CSV and store on new folder: csv_files\n",
    "    #df_xco2.to_csv('csv_files'+'/'+ data.Sensor+'_xco2_'+ date+'_.txt', index= False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1506ef2f",
   "metadata": {
    "id": "1506ef2f"
   },
   "source": [
    "# Testing: Single file transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6be0ec39",
   "metadata": {
    "id": "6be0ec39"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Xco2</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>xco2_quality_flag</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>392.113342</td>\n",
       "      <td>23.887823</td>\n",
       "      <td>-165.322113</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-15 00:14:23.340</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>395.041077</td>\n",
       "      <td>23.906345</td>\n",
       "      <td>-165.327133</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-15 00:14:23.740</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>394.904938</td>\n",
       "      <td>23.969070</td>\n",
       "      <td>-165.351288</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-15 00:14:24.730</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>393.841583</td>\n",
       "      <td>23.980486</td>\n",
       "      <td>-165.347305</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-15 00:14:25.040</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>395.470764</td>\n",
       "      <td>23.999025</td>\n",
       "      <td>-165.352356</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-15 00:14:25.340</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20998</th>\n",
       "      <td>392.676025</td>\n",
       "      <td>47.035870</td>\n",
       "      <td>-159.644638</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-15 23:25:51.750</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20999</th>\n",
       "      <td>393.736023</td>\n",
       "      <td>47.032795</td>\n",
       "      <td>-159.626373</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-15 23:25:51.760</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21001</th>\n",
       "      <td>393.991211</td>\n",
       "      <td>47.026253</td>\n",
       "      <td>-159.590057</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-15 23:25:51.780</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21002</th>\n",
       "      <td>393.972900</td>\n",
       "      <td>47.053600</td>\n",
       "      <td>-159.653290</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-15 23:25:52.050</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21004</th>\n",
       "      <td>394.014282</td>\n",
       "      <td>47.047325</td>\n",
       "      <td>-159.616806</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-15 23:25:52.070</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14229 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Xco2   Latitude   Longitude  xco2_quality_flag  \\\n",
       "7      392.113342  23.887823 -165.322113                  0   \n",
       "12     395.041077  23.906345 -165.327133                  0   \n",
       "22     394.904938  23.969070 -165.351288                  0   \n",
       "26     393.841583  23.980486 -165.347305                  0   \n",
       "33     395.470764  23.999025 -165.352356                  0   \n",
       "...           ...        ...         ...                ...   \n",
       "20998  392.676025  47.035870 -159.644638                  0   \n",
       "20999  393.736023  47.032795 -159.626373                  0   \n",
       "21001  393.991211  47.026253 -159.590057                  0   \n",
       "21002  393.972900  47.053600 -159.653290                  0   \n",
       "21004  394.014282  47.047325 -159.616806                  0   \n",
       "\n",
       "                     DateTime  Year  Month  Day  \n",
       "7     2014-09-15 00:14:23.340  2014      9   15  \n",
       "12    2014-09-15 00:14:23.740  2014      9   15  \n",
       "22    2014-09-15 00:14:24.730  2014      9   15  \n",
       "26    2014-09-15 00:14:25.040  2014      9   15  \n",
       "33    2014-09-15 00:14:25.340  2014      9   15  \n",
       "...                       ...   ...    ...  ...  \n",
       "20998 2014-09-15 23:25:51.750  2014      9   15  \n",
       "20999 2014-09-15 23:25:51.760  2014      9   15  \n",
       "21001 2014-09-15 23:25:51.780  2014      9   15  \n",
       "21002 2014-09-15 23:25:52.050  2014      9   15  \n",
       "21004 2014-09-15 23:25:52.070  2014      9   15  \n",
       "\n",
       "[14229 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convHdf(file_names[9])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c367997",
   "metadata": {
    "id": "4c367997"
   },
   "source": [
    "## Concatenate all files into one master dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "221c73b6",
   "metadata": {
    "id": "221c73b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\data_netcdf\\oco2_LtCO2_141023_B11014Ar_230329200159s.SUB.nc4  is missing xco2 data\n",
      "data\\data_netcdf\\oco2_LtCO2_150129_B11014Ar_230322180219s.SUB.nc4  is missing xco2 data\n",
      "data\\data_netcdf\\oco2_LtCO2_151026_B11014Ar_230222200809s.SUB.nc4  is missing xco2 data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Xco2</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>xco2_quality_flag</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>389.764069</td>\n",
       "      <td>81.391083</td>\n",
       "      <td>-74.009087</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-06 13:48:24.760</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>388.759888</td>\n",
       "      <td>81.399887</td>\n",
       "      <td>-73.966896</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-06 13:48:24.770</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>389.952484</td>\n",
       "      <td>81.412910</td>\n",
       "      <td>-74.224922</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-06 13:48:25.370</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>392.939789</td>\n",
       "      <td>77.179932</td>\n",
       "      <td>-66.461746</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-06 15:25:20.030</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>389.582825</td>\n",
       "      <td>77.195396</td>\n",
       "      <td>-66.521400</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-06 15:25:20.330</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>391.755737</td>\n",
       "      <td>77.201897</td>\n",
       "      <td>-66.670692</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-06 15:25:20.710</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>390.422668</td>\n",
       "      <td>77.206406</td>\n",
       "      <td>-66.626122</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-06 15:25:20.720</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>391.380463</td>\n",
       "      <td>77.221825</td>\n",
       "      <td>-66.686157</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-06 15:25:21.020</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>390.483276</td>\n",
       "      <td>77.241692</td>\n",
       "      <td>-66.701355</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-06 15:25:21.330</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>393.851318</td>\n",
       "      <td>77.252609</td>\n",
       "      <td>-66.806648</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-06 15:25:21.720</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Xco2   Latitude  Longitude  xco2_quality_flag  \\\n",
       "0  389.764069  81.391083 -74.009087                  0   \n",
       "1  388.759888  81.399887 -73.966896                  0   \n",
       "2  389.952484  81.412910 -74.224922                  0   \n",
       "3  392.939789  77.179932 -66.461746                  0   \n",
       "4  389.582825  77.195396 -66.521400                  0   \n",
       "5  391.755737  77.201897 -66.670692                  0   \n",
       "6  390.422668  77.206406 -66.626122                  0   \n",
       "7  391.380463  77.221825 -66.686157                  0   \n",
       "8  390.483276  77.241692 -66.701355                  0   \n",
       "9  393.851318  77.252609 -66.806648                  0   \n",
       "\n",
       "                 DateTime  Year  Month  Day  \n",
       "0 2014-09-06 13:48:24.760  2014      9    6  \n",
       "1 2014-09-06 13:48:24.770  2014      9    6  \n",
       "2 2014-09-06 13:48:25.370  2014      9    6  \n",
       "3 2014-09-06 15:25:20.030  2014      9    6  \n",
       "4 2014-09-06 15:25:20.330  2014      9    6  \n",
       "5 2014-09-06 15:25:20.710  2014      9    6  \n",
       "6 2014-09-06 15:25:20.720  2014      9    6  \n",
       "7 2014-09-06 15:25:21.020  2014      9    6  \n",
       "8 2014-09-06 15:25:21.330  2014      9    6  \n",
       "9 2014-09-06 15:25:21.720  2014      9    6  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using Function to READ FILES from the direcotry and convert all netCDF files to csv/txt \n",
    "\n",
    "#initializing   \n",
    "df_all = convHdf(file_names[0])\n",
    "\n",
    "for j in range(1, len(file_names)):\n",
    "  \n",
    "       # EG to read FIRST dataset from THE DIRECTORY       \n",
    "        df=convHdf(file_names[j], j)\n",
    "        df_all = pd.concat([df_all,df],ignore_index=True)\n",
    "\n",
    "df_all[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89d824e0",
   "metadata": {},
   "source": [
    "## Write dataframe to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "849effab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(\"data_master_csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "639678bd",
   "metadata": {},
   "source": [
    "## Checkout breakdown of readings per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "194416c5",
   "metadata": {
    "id": "194416c5"
   },
   "outputs": [],
   "source": [
    "df_grouped=df_all.groupby(['Year','Month','Day']).size().reset_index().rename(columns={0:'count'})\n",
    "df_grouped.to_csv(\"OVERVIEW_data_master\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Conversion_NETCDF_to_CSV_2019_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
