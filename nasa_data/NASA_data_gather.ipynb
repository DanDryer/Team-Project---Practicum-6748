{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c541a020",
   "metadata": {
    "id": "c541a020"
   },
   "source": [
    "# Pulling NETCDF files from NASA, Aggregating files, filtering, and Converting to .CSV\n",
    "### Objective:\n",
    "- There is a specific way to pull down data from NASA\n",
    "- netCDF files are large size and can take much more resourced to compute for data analysis\n",
    "- Should convert netCDF to CSV format and filter to reduce the size of data\n",
    "\n",
    "\n",
    "### STEPS: \n",
    "* Ensure that you have the correct credentials to access the data\n",
    "* Setup your environment to work with wget\n",
    "* Use wget to pull down daily files from a subset link provided by NASA \n",
    "* Do some preliminary filtering \n",
    "* Concatenate the data files\n",
    "* final output: csv files for future analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd3e4835",
   "metadata": {},
   "source": [
    "### Ensure that you have the following completed: \n",
    "* You have registered for an EarthData Login Profile: https://wiki.earthdata.nasa.gov/display/EL/How+To+Register+For+an+EarthData+Login+Profile\n",
    "* You have authorized \"NASA GESDISC DATA ARCHIVE\": https://disc.gsfc.nasa.gov/earthdata-login\n",
    "* Generate the prerequisite files needed for the wget tool: https://disc.gsfc.nasa.gov/information/howto?title=How%20to%20Generate%20Earthdata%20Prerequisite%20Files\n",
    "* Download the wget tool \n",
    "    * This is a little tricky, I was running into authorization issues on the NASA site using the most recent version of wget. I found successs using version 1.19.2\n",
    "    * You can download from: https://eternallybored.org/misc/wget/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f39f5826",
   "metadata": {},
   "source": [
    "## Subset the data as needed\n",
    "\n",
    "At the moment, the dataset that we are using can be found here: https://disc.gsfc.nasa.gov/api/jobs/results/6480ba7f9c692c7cd8c4a794\n",
    "\n",
    "A handy subsetting tool is available\n",
    "\n",
    " ![alt text](Images\\subset_image.jpg)\n",
    "\n",
    " From there, you can subset on a specific region, date range, etc. \n",
    "\n",
    " ![Alt text](Images\\subset_region.jpg)\n",
    "\n",
    " Once you are finished, you will be directed to this screen where you need to download the list of links provided. The list should be as long as the the number of days that you are subsetting from since each link directs to a data file for one day of readings.\n",
    "\n",
    " ![Alt text](Images\\download_links_2014_2016.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54093d0e",
   "metadata": {},
   "source": [
    "## Get the data with wget\n",
    "\n",
    "If you have correctly completed all the previous steps, this should work for you\n",
    "* Open a command prompt on your PC\n",
    "* Set your working directory to the location of your link list as well as the .dodsrc file you created (see the prequisite files mentioned above)\n",
    "* From the command line, run: \n",
    "\n",
    "wget --load-cookies <path of .urs_cookies file> --save-cookies <path of .urs_cookies file> --keep-session-cookies --user=< YOURUSERNAME > --ask-password -P <folder you want to save the data to> --content-disposition -i <link list .txt>\n",
    "\n",
    "For example:\n",
    "\n",
    "wget --load-cookies C:\\Users\\badger\\ .urs_cookies --save-cookies C:\\Users\\badger\\ .urs_cookies --keep-session-cookies --user=bbadger --ask-password -P data_2014/ --content-disposition -i 2014_subset_data_links.txt\n",
    "\n",
    "* it takes me roughly 5 seconds to download one file, for reference\n",
    "\n",
    "You should now have your netcdf files downloaded\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8c014d2",
   "metadata": {},
   "source": [
    "## Convert and concatenate multiple netCDF files to master CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a680156",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1652679167658,
     "user": {
      "displayName": "Kikipessa Doll",
      "userId": "16203075636725704864"
     },
     "user_tz": 420
    },
    "id": "9a680156"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import netCDF4 as nc\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab238c96",
   "metadata": {
    "id": "ab238c96"
   },
   "source": [
    "## Path to NETCDF files and open counties shapefile\n",
    "- Locate the downloaded netcdf files directory in pc directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fa1e6c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 214,
     "status": "error",
     "timestamp": 1652679503238,
     "user": {
      "displayName": "Kikipessa Doll",
      "userId": "16203075636725704864"
     },
     "user_tz": 420
    },
    "id": "1fa1e6c8",
    "outputId": "7e95debd-55c7-47b2-e114-2120a08c6304"
   },
   "outputs": [],
   "source": [
    "counties = gpd.GeoDataFrame.from_file(r\"C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\us-county-boundaries\\us-county-boundaries.shp\")\n",
    "path_a= (r\"C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_all\")\n",
    "\n",
    "# Collect the path of each individual files\n",
    "file_names= []\n",
    "\n",
    "for file in os.listdir(path_a):\n",
    "    # Check whether file ==.nc4 type\n",
    "    if file.endswith(\".nc4\"):\n",
    "        file_path = f\"{path_a}\\{file}\"\n",
    "      \n",
    "        # Store the path location of each individual files\n",
    "        file_names.append(file_path)\n",
    "        \n",
    "        \n",
    "# check first 10 files path\n",
    "#file_names[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd2ad694",
   "metadata": {
    "id": "cd2ad694"
   },
   "source": [
    "# Check the total files in the DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de96547b",
   "metadata": {
    "id": "de96547b",
    "outputId": "2c929c0a-183e-447c-d3cb-2bdb0dba76ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TotalFiles:  2943\n"
     ]
    }
   ],
   "source": [
    "countFiles=0\n",
    "\n",
    "for j in file_names:\n",
    "    if j.endswith(\".nc4\"):\n",
    "        countFiles+=1\n",
    "        #print(j)\n",
    "        \n",
    "print('\\nTotalFiles: ', countFiles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4a27bf7",
   "metadata": {
    "id": "e4a27bf7"
   },
   "source": [
    "# Example: \n",
    "### Opening a single file in netCDF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71c5c0f8",
   "metadata": {
    "id": "71c5c0f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sounding_id_idx',\n",
       " 'longitude',\n",
       " 'latitude',\n",
       " 'time',\n",
       " 'epoch_dimension',\n",
       " 'co2_profile_apriori',\n",
       " 'date',\n",
       " 'file_index',\n",
       " 'pressure_levels',\n",
       " 'pressure_weight',\n",
       " 'sensor_zenith_angle',\n",
       " 'solar_zenith_angle',\n",
       " 'vertex_latitude',\n",
       " 'vertex_longitude',\n",
       " 'xco2',\n",
       " 'xco2_apriori',\n",
       " 'xco2_averaging_kernel',\n",
       " 'xco2_qf_bitflag',\n",
       " 'xco2_quality_flag',\n",
       " 'xco2_uncertainty',\n",
       " 'sounding_id',\n",
       " 'levels',\n",
       " 'vertices']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xco2= nc.Dataset(file_names[0])\n",
    "list(df_xco2.variables.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f1aa9ba",
   "metadata": {
    "id": "8f1aa9ba"
   },
   "source": [
    "## Function for DateTime format change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4133c955",
   "metadata": {
    "id": "4133c955"
   },
   "outputs": [],
   "source": [
    "# DATE time function\n",
    "def conv_date(d):\n",
    "    return datetime.strptime(str(d), '%Y%m%d%H%M%S%f')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7d135ca",
   "metadata": {
    "id": "d7d135ca"
   },
   "source": [
    "### Function for Dataframe building\n",
    "- Ensure that the netcdf file has an xco2 column, if not, then we will try to redownload that file and append it to the final dataframe\n",
    "- Convert the date/time columns to the correct type\n",
    "- Create a 'coords' column populated with Point types so that we can match state, county, geoid to each xco2 reading\n",
    "- Only use variables of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bfa8617",
   "metadata": {
    "id": "5bfa8617"
   },
   "outputs": [],
   "source": [
    "# FUNCTION to convert data\n",
    "\n",
    "#keeping track of any missing data \n",
    "missing_data=[]\n",
    "\n",
    "#function takes the a file path and whether we want to filter out the quallity flagged data\n",
    "def convHdf(path_file):\n",
    "\n",
    "    #opening the file as netcdf\n",
    "    data= nc.Dataset(path_file)\n",
    "\n",
    "    #if xco2 field is empty, report, add to missing_data, and continue\n",
    "    if 'xco2' not in data.variables.keys():\n",
    "        #print(path_file,\" is missing xco2 data\")\n",
    "        missing_data.append(path_file)\n",
    "        return \n",
    "\n",
    "    #creating an empty dataframe to populate\n",
    "    df_xco2= pd.DataFrame()\n",
    "\n",
    "    #creating and populating columns\n",
    "    df_xco2['xco2']= data.variables['xco2'][:]\n",
    "    df_xco2['Latitude']= data.variables['latitude'][:]\n",
    "    df_xco2['Longitude']= data.variables['longitude'][:] \n",
    "    df_xco2['xco2_quality_flag']= data.variables['xco2_quality_flag'][:]\n",
    "\n",
    "    # Convert soundingID to datetime format\n",
    "    df_xco2['DateTime']= data.variables['sounding_id'][:]\n",
    "    # Applying the conv_date function we created previously\n",
    "    df_xco2['DateTime']= df_xco2['DateTime'].apply(conv_date)\n",
    "    df_xco2['DateTime']= pd.to_datetime(df_xco2['DateTime'])\n",
    "    \n",
    "    # Creating specific date columns\n",
    "    df_xco2['Year']= df_xco2['DateTime'].dt.year\n",
    "    df_xco2['Month']= df_xco2['DateTime'].dt.month\n",
    "    df_xco2['Day']= df_xco2['DateTime'].dt.day\n",
    "\n",
    "    #Creating a point field from longitude and latitude for matching the geoid,county,state info from the .shp geo data file\n",
    "    df_xco2['coords'] = list(zip(df_xco2['Longitude'],df_xco2['Latitude']))\n",
    "    df_xco2['coords'] = df_xco2['coords'].apply(Point)\n",
    "    points = gpd.GeoDataFrame(df_xco2, geometry='coords', crs=counties.crs)\n",
    "    df_xco2 = gpd.tools.sjoin(points, counties, predicate=\"within\", how='left')\n",
    "\n",
    "    #dropping any rows that have not been assigned a county name, we can assume the reading was taken outside of the united states\n",
    "    df_xco2.dropna(subset=['name'], inplace=True)\n",
    "\n",
    "    #renaming and choosing only the fields that we want\n",
    "    df_xco2.rename(columns={\"name\": \"county_name\"},inplace=True)\n",
    "    df_xco2=df_xco2[[\"county_name\",\"state_name\",\"geoid\",\"DateTime\",\"Year\",\"Month\",\"Day\",\"Latitude\",\"Longitude\",\"xco2\",\"xco2_quality_flag\",]]\n",
    "\n",
    "    return df_xco2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1506ef2f",
   "metadata": {
    "id": "1506ef2f"
   },
   "source": [
    "# Testing: Single file transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c8358ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9617 non filtered rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county_name</th>\n",
       "      <th>state_name</th>\n",
       "      <th>geoid</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>xco2</th>\n",
       "      <th>xco2_quality_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>Hancock</td>\n",
       "      <td>Maine</td>\n",
       "      <td>23009</td>\n",
       "      <td>2014-09-07 17:37:46.360</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>44.158154</td>\n",
       "      <td>-68.420464</td>\n",
       "      <td>393.300018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>Hancock</td>\n",
       "      <td>Maine</td>\n",
       "      <td>23009</td>\n",
       "      <td>2014-09-07 17:37:46.370</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>44.154636</td>\n",
       "      <td>-68.406906</td>\n",
       "      <td>395.017761</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>Hancock</td>\n",
       "      <td>Maine</td>\n",
       "      <td>23009</td>\n",
       "      <td>2014-09-07 17:37:46.770</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>44.174461</td>\n",
       "      <td>-68.413887</td>\n",
       "      <td>393.613922</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>Hancock</td>\n",
       "      <td>Maine</td>\n",
       "      <td>23009</td>\n",
       "      <td>2014-09-07 17:37:49.340</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>44.343330</td>\n",
       "      <td>-68.511208</td>\n",
       "      <td>393.447845</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>Hancock</td>\n",
       "      <td>Maine</td>\n",
       "      <td>23009</td>\n",
       "      <td>2014-09-07 17:37:49.350</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>44.340012</td>\n",
       "      <td>-68.497513</td>\n",
       "      <td>391.911407</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>Hancock</td>\n",
       "      <td>Maine</td>\n",
       "      <td>23009</td>\n",
       "      <td>2014-09-07 17:37:49.750</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>44.359821</td>\n",
       "      <td>-68.504669</td>\n",
       "      <td>391.107697</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>Hancock</td>\n",
       "      <td>Maine</td>\n",
       "      <td>23009</td>\n",
       "      <td>2014-09-07 17:37:50.330</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>44.406162</td>\n",
       "      <td>-68.546303</td>\n",
       "      <td>395.608612</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>Hancock</td>\n",
       "      <td>Maine</td>\n",
       "      <td>23009</td>\n",
       "      <td>2014-09-07 17:37:50.710</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>44.432350</td>\n",
       "      <td>-68.581123</td>\n",
       "      <td>393.809479</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>Hancock</td>\n",
       "      <td>Maine</td>\n",
       "      <td>23009</td>\n",
       "      <td>2014-09-07 17:37:50.720</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>44.429253</td>\n",
       "      <td>-68.567215</td>\n",
       "      <td>392.958099</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>Hancock</td>\n",
       "      <td>Maine</td>\n",
       "      <td>23009</td>\n",
       "      <td>2014-09-07 17:37:50.730</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>44.426067</td>\n",
       "      <td>-68.553291</td>\n",
       "      <td>391.530762</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    county_name state_name  geoid                DateTime  Year  Month  Day  \\\n",
       "503     Hancock      Maine  23009 2014-09-07 17:37:46.360  2014      9    7   \n",
       "504     Hancock      Maine  23009 2014-09-07 17:37:46.370  2014      9    7   \n",
       "505     Hancock      Maine  23009 2014-09-07 17:37:46.770  2014      9    7   \n",
       "506     Hancock      Maine  23009 2014-09-07 17:37:49.340  2014      9    7   \n",
       "507     Hancock      Maine  23009 2014-09-07 17:37:49.350  2014      9    7   \n",
       "508     Hancock      Maine  23009 2014-09-07 17:37:49.750  2014      9    7   \n",
       "509     Hancock      Maine  23009 2014-09-07 17:37:50.330  2014      9    7   \n",
       "510     Hancock      Maine  23009 2014-09-07 17:37:50.710  2014      9    7   \n",
       "511     Hancock      Maine  23009 2014-09-07 17:37:50.720  2014      9    7   \n",
       "512     Hancock      Maine  23009 2014-09-07 17:37:50.730  2014      9    7   \n",
       "\n",
       "      Latitude  Longitude        xco2  xco2_quality_flag  \n",
       "503  44.158154 -68.420464  393.300018                  0  \n",
       "504  44.154636 -68.406906  395.017761                  0  \n",
       "505  44.174461 -68.413887  393.613922                  0  \n",
       "506  44.343330 -68.511208  393.447845                  0  \n",
       "507  44.340012 -68.497513  391.911407                  1  \n",
       "508  44.359821 -68.504669  391.107697                  1  \n",
       "509  44.406162 -68.546303  395.608612                  1  \n",
       "510  44.432350 -68.581123  393.809479                  0  \n",
       "511  44.429253 -68.567215  392.958099                  0  \n",
       "512  44.426067 -68.553291  391.530762                  0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test=convHdf(file_names[1])\n",
    "\n",
    "print(len(test),\"non filtered rows\")\n",
    "display(test[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c367997",
   "metadata": {
    "id": "4c367997"
   },
   "source": [
    "## Concatenate all files into one master dataframe without quality flags filtered and identify what days data may be missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "221c73b6",
   "metadata": {
    "id": "221c73b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_all\\oco2_LtCO2_141023_B11014Ar_230329200159s.SUB.nc4  is missing xco2 data\n",
      "C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_all\\oco2_LtCO2_150129_B11014Ar_230322180219s.SUB.nc4  is missing xco2 data\n",
      "C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_all\\oco2_LtCO2_151026_B11014Ar_230222200809s.SUB.nc4  is missing xco2 data\n",
      "C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_all\\oco2_LtCO2_161229_B11014Ar_230111184801s.SUB.nc4  is missing xco2 data\n",
      "C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_all\\oco2_LtCO2_171024_B11014Ar_221212230141s.SUB.nc4  is missing xco2 data\n",
      "C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_all\\oco2_LtCO2_181110_B11014Ar_221019173153s.SUB.nc4  is missing xco2 data\n",
      "C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_all\\oco2_LtCO2_190304_B11014Ar_221012170715s.SUB.nc4  is missing xco2 data\n",
      "C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_all\\oco2_LtCO2_201230_B11014Ar_220729012754s.SUB.nc4  is missing xco2 data\n",
      "C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_all\\oco2_LtCO2_210201_B11014Ar_220729004203s.SUB.nc4  is missing xco2 data\n",
      "C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_all\\oco2_LtCO2_220506_B11014Ar_220822213813s.SUB.nc4  is missing xco2 data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county_name</th>\n",
       "      <th>state_name</th>\n",
       "      <th>geoid</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>xco2</th>\n",
       "      <th>xco2_quality_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Moore</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>37125</td>\n",
       "      <td>2014-09-06 18:30:51.370</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>35.101131</td>\n",
       "      <td>-79.464561</td>\n",
       "      <td>388.310669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Moore</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>37125</td>\n",
       "      <td>2014-09-06 18:30:51.730</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>35.141167</td>\n",
       "      <td>-79.513474</td>\n",
       "      <td>385.072388</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Moore</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>37125</td>\n",
       "      <td>2014-09-06 18:30:52.080</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>35.135918</td>\n",
       "      <td>-79.465286</td>\n",
       "      <td>388.949951</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Moore</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>37125</td>\n",
       "      <td>2014-09-06 18:30:52.380</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>35.155876</td>\n",
       "      <td>-79.470993</td>\n",
       "      <td>386.402069</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Moore</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>37125</td>\n",
       "      <td>2014-09-06 18:30:53.330</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>35.240818</td>\n",
       "      <td>-79.542213</td>\n",
       "      <td>391.193634</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  county_name      state_name  geoid                DateTime  Year  Month  \\\n",
       "0       Moore  North Carolina  37125 2014-09-06 18:30:51.370  2014      9   \n",
       "1       Moore  North Carolina  37125 2014-09-06 18:30:51.730  2014      9   \n",
       "2       Moore  North Carolina  37125 2014-09-06 18:30:52.080  2014      9   \n",
       "3       Moore  North Carolina  37125 2014-09-06 18:30:52.380  2014      9   \n",
       "4       Moore  North Carolina  37125 2014-09-06 18:30:53.330  2014      9   \n",
       "\n",
       "   Day   Latitude  Longitude        xco2  xco2_quality_flag  \n",
       "0    6  35.101131 -79.464561  388.310669                  1  \n",
       "1    6  35.141167 -79.513474  385.072388                  1  \n",
       "2    6  35.135918 -79.465286  388.949951                  1  \n",
       "3    6  35.155876 -79.470993  386.402069                  1  \n",
       "4    6  35.240818 -79.542213  391.193634                  1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county_name</th>\n",
       "      <th>state_name</th>\n",
       "      <th>geoid</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>xco2</th>\n",
       "      <th>xco2_quality_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14960030</th>\n",
       "      <td>North Slope</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>02185</td>\n",
       "      <td>2023-03-31 22:48:39.780</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>68.875946</td>\n",
       "      <td>-163.871399</td>\n",
       "      <td>414.965546</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14960031</th>\n",
       "      <td>North Slope</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>02185</td>\n",
       "      <td>2023-03-31 22:48:40.040</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>68.943138</td>\n",
       "      <td>-164.020432</td>\n",
       "      <td>401.649536</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14960032</th>\n",
       "      <td>North Slope</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>02185</td>\n",
       "      <td>2023-03-31 22:48:40.050</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>68.941261</td>\n",
       "      <td>-163.990616</td>\n",
       "      <td>402.592468</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14960033</th>\n",
       "      <td>North Slope</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>02185</td>\n",
       "      <td>2023-03-31 22:48:40.070</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>68.936943</td>\n",
       "      <td>-163.931183</td>\n",
       "      <td>403.559906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14960034</th>\n",
       "      <td>North Slope</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>02185</td>\n",
       "      <td>2023-03-31 22:48:40.080</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>68.934380</td>\n",
       "      <td>-163.901611</td>\n",
       "      <td>406.664581</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          county_name state_name  geoid                DateTime  Year  Month  \\\n",
       "14960030  North Slope     Alaska  02185 2023-03-31 22:48:39.780  2023      3   \n",
       "14960031  North Slope     Alaska  02185 2023-03-31 22:48:40.040  2023      3   \n",
       "14960032  North Slope     Alaska  02185 2023-03-31 22:48:40.050  2023      3   \n",
       "14960033  North Slope     Alaska  02185 2023-03-31 22:48:40.070  2023      3   \n",
       "14960034  North Slope     Alaska  02185 2023-03-31 22:48:40.080  2023      3   \n",
       "\n",
       "          Day   Latitude   Longitude        xco2  xco2_quality_flag  \n",
       "14960030   31  68.875946 -163.871399  414.965546                  1  \n",
       "14960031   31  68.943138 -164.020432  401.649536                  1  \n",
       "14960032   31  68.941261 -163.990616  402.592468                  1  \n",
       "14960033   31  68.936943 -163.931183  403.559906                  1  \n",
       "14960034   31  68.934380 -163.901611  406.664581                  1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using Function to READ FILES from the direcotry and convert all netCDF files to csv/txt \n",
    "\n",
    "#initializing to get the column headings\n",
    "df_xco2 = convHdf(file_names[0])\n",
    "\n",
    "#for each file in file_names\n",
    "for j in range(1, len(file_names)):\n",
    "  \n",
    "       # Run the previously created function - be sure to define whether you want to perform quality filtering or not     \n",
    "        df=convHdf(file_names[j])\n",
    "        df_xco2 = pd.concat([df_xco2,df],ignore_index=True)\n",
    "\n",
    "\n",
    "display(df_xco2.head())\n",
    "display(df_xco2.tail())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68abaf69",
   "metadata": {},
   "source": [
    "## Identify missing data and search through the nasa subset function\n",
    "\n",
    "### For whatever reason, I fount that these days in the NASA database are missing xco2 data - something we should make note of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc1f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_b= (r\"C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_missing\")\n",
    "\n",
    "# Collect the path of each individual files\n",
    "file_names_missing= []\n",
    "\n",
    "for file in os.listdir(path_b):\n",
    "    # Check whether file ==.nc4 type\n",
    "    if file.endswith(\".nc4\"):\n",
    "        file_path = f\"{path_a}\\{file}\"\n",
    "      \n",
    "        # Store the path location of each individual files\n",
    "        file_names_missing.append(file_path)\n",
    "        \n",
    "        \n",
    "# check first 10 files path\n",
    "print(file_names_missing[:10])\n",
    "\n",
    "#for each file in file_names\n",
    "for j in range(1, len(file_names_missing)):\n",
    "        missing_data= nc.Dataset(file_names_missing[j])\n",
    "\n",
    "        print(missing_data.variables.keys())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92dee572",
   "metadata": {},
   "source": [
    "## Write base data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "9a182c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path=r'C:/Users/ddrye/OneDrive/Documents/OMSA_Program/OMSA 2023\\Summer2023/Practicum/off_git/data/'\n",
    "\n",
    "df_xco2.to_csv(local_path+'OCO2_BASE_2014-2023_V1.csv',index=False) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f906082a",
   "metadata": {},
   "source": [
    "## Filter out bad quality flags and calculate stats and rates of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "51117d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variables(df,filter_quality=True):\n",
    "\n",
    "    if filter_quality==True:\n",
    "        df=df[df['xco2_quality_flag']==0]\n",
    "    \n",
    "    #Only gettin years where we have values for the entire year, not just a few months\n",
    "    df = df.loc[(df['Year'] < 2023) & (df['Year'] > 2014) ]\n",
    "    \n",
    "    #add new variables\n",
    "    counts=df.groupby(['geoid','county_name','state_name','Year'], as_index=False)[\"xco2\"].size().rename(columns={'size':'readings_count'})\n",
    "    mean=df.groupby(['geoid','Year'], as_index=False)[\"xco2\"].mean().rename(columns={'xco2':'avg_xco2'})\n",
    "    std_deviation=df.groupby(['geoid','Year'], as_index=False)[\"xco2\"].std().rename(columns={'xco2':'stddev_xco2'})\n",
    "    \n",
    "    intermediate_df=pd.merge(counts, mean[['geoid','Year','avg_xco2']], on=['geoid','Year'])\n",
    "    pct_change = (intermediate_df.groupby('geoid')['avg_xco2']\n",
    "                                  .apply(pd.Series.pct_change) + 1).rename('pct_change').reset_index()\n",
    "\n",
    "    intermediate_df=pd.merge(intermediate_df, pct_change['pct_change'], left_index=True, right_index=True)\n",
    "    intermediate_df=pd.merge(intermediate_df, std_deviation[['geoid','Year','stddev_xco2']], on=['geoid','Year'])\n",
    "\n",
    "    intermediate_df[\"delta\"] = intermediate_df.groupby(['geoid'])['avg_xco2'].diff()\n",
    "    intermediate_df[\"total_delta\"] = intermediate_df.groupby(['geoid'])['delta'].cumsum()\n",
    "    \n",
    "\n",
    "    final_df=intermediate_df[[\"geoid\",\"county_name\",\"state_name\",\"Year\",\"readings_count\",\"stddev_xco2\",\"avg_xco2\",\"delta\",\"total_delta\",\"pct_change\"]]\n",
    "            \n",
    "    return final_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e8da5e1",
   "metadata": {},
   "source": [
    "## Apply the new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "df80b7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geoid</th>\n",
       "      <th>county_name</th>\n",
       "      <th>state_name</th>\n",
       "      <th>Year</th>\n",
       "      <th>readings_count</th>\n",
       "      <th>stddev_xco2</th>\n",
       "      <th>avg_xco2</th>\n",
       "      <th>delta</th>\n",
       "      <th>total_delta</th>\n",
       "      <th>pct_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01001</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2015</td>\n",
       "      <td>43</td>\n",
       "      <td>0.945703</td>\n",
       "      <td>396.257507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01001</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2016</td>\n",
       "      <td>268</td>\n",
       "      <td>1.087421</td>\n",
       "      <td>403.380249</td>\n",
       "      <td>7.122742</td>\n",
       "      <td>7.122742</td>\n",
       "      <td>1.017975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01001</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2017</td>\n",
       "      <td>67</td>\n",
       "      <td>1.190920</td>\n",
       "      <td>403.876709</td>\n",
       "      <td>0.496460</td>\n",
       "      <td>7.619202</td>\n",
       "      <td>1.001231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01001</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2018</td>\n",
       "      <td>437</td>\n",
       "      <td>2.605162</td>\n",
       "      <td>408.490875</td>\n",
       "      <td>4.614166</td>\n",
       "      <td>12.233368</td>\n",
       "      <td>1.011425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01001</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2019</td>\n",
       "      <td>355</td>\n",
       "      <td>1.044961</td>\n",
       "      <td>410.446869</td>\n",
       "      <td>1.955994</td>\n",
       "      <td>14.189362</td>\n",
       "      <td>1.004788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   geoid county_name state_name  Year  readings_count  stddev_xco2  \\\n",
       "0  01001     Autauga    Alabama  2015              43     0.945703   \n",
       "1  01001     Autauga    Alabama  2016             268     1.087421   \n",
       "2  01001     Autauga    Alabama  2017              67     1.190920   \n",
       "3  01001     Autauga    Alabama  2018             437     2.605162   \n",
       "4  01001     Autauga    Alabama  2019             355     1.044961   \n",
       "\n",
       "     avg_xco2     delta  total_delta  pct_change  \n",
       "0  396.257507       NaN          NaN         NaN  \n",
       "1  403.380249  7.122742     7.122742    1.017975  \n",
       "2  403.876709  0.496460     7.619202    1.001231  \n",
       "3  408.490875  4.614166    12.233368    1.011425  \n",
       "4  410.446869  1.955994    14.189362    1.004788  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geoid</th>\n",
       "      <th>county_name</th>\n",
       "      <th>state_name</th>\n",
       "      <th>Year</th>\n",
       "      <th>readings_count</th>\n",
       "      <th>stddev_xco2</th>\n",
       "      <th>avg_xco2</th>\n",
       "      <th>delta</th>\n",
       "      <th>total_delta</th>\n",
       "      <th>pct_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21781</th>\n",
       "      <td>56045</td>\n",
       "      <td>Weston</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2018</td>\n",
       "      <td>449</td>\n",
       "      <td>2.742509</td>\n",
       "      <td>406.202148</td>\n",
       "      <td>0.424316</td>\n",
       "      <td>6.014740</td>\n",
       "      <td>1.001046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21782</th>\n",
       "      <td>56045</td>\n",
       "      <td>Weston</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2019</td>\n",
       "      <td>270</td>\n",
       "      <td>0.492759</td>\n",
       "      <td>410.860779</td>\n",
       "      <td>4.658630</td>\n",
       "      <td>10.673370</td>\n",
       "      <td>1.011469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21783</th>\n",
       "      <td>56045</td>\n",
       "      <td>Weston</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020</td>\n",
       "      <td>625</td>\n",
       "      <td>2.966075</td>\n",
       "      <td>412.226837</td>\n",
       "      <td>1.366058</td>\n",
       "      <td>12.039429</td>\n",
       "      <td>1.003325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21784</th>\n",
       "      <td>56045</td>\n",
       "      <td>Weston</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2021</td>\n",
       "      <td>69</td>\n",
       "      <td>1.068860</td>\n",
       "      <td>413.128815</td>\n",
       "      <td>0.901978</td>\n",
       "      <td>12.941406</td>\n",
       "      <td>1.002188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21785</th>\n",
       "      <td>56045</td>\n",
       "      <td>Weston</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2022</td>\n",
       "      <td>350</td>\n",
       "      <td>1.919243</td>\n",
       "      <td>418.569550</td>\n",
       "      <td>5.440735</td>\n",
       "      <td>18.382141</td>\n",
       "      <td>1.013170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       geoid county_name state_name  Year  readings_count  stddev_xco2  \\\n",
       "21781  56045      Weston    Wyoming  2018             449     2.742509   \n",
       "21782  56045      Weston    Wyoming  2019             270     0.492759   \n",
       "21783  56045      Weston    Wyoming  2020             625     2.966075   \n",
       "21784  56045      Weston    Wyoming  2021              69     1.068860   \n",
       "21785  56045      Weston    Wyoming  2022             350     1.919243   \n",
       "\n",
       "         avg_xco2     delta  total_delta  pct_change  \n",
       "21781  406.202148  0.424316     6.014740    1.001046  \n",
       "21782  410.860779  4.658630    10.673370    1.011469  \n",
       "21783  412.226837  1.366058    12.039429    1.003325  \n",
       "21784  413.128815  0.901978    12.941406    1.002188  \n",
       "21785  418.569550  5.440735    18.382141    1.013170  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_copy=df_xco2\n",
    "df_xco2_v1=variables(df_copy,filter_quality=True)\n",
    "\n",
    "display(df_xco2_v1.head())\n",
    "display(df_xco2_v1.tail())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89d824e0",
   "metadata": {},
   "source": [
    "## Write cleaned df to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "849effab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xco2_v1.to_csv(local_path+'OCO2_CLEANED_2015-2022_V1.csv',index=False) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Conversion_NETCDF_to_CSV_2019_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
