{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c541a020",
   "metadata": {
    "id": "c541a020"
   },
   "source": [
    "# Pulling NETCDF files from NASA, Aggregating files, Converting to .CSV\n",
    "### Objective:\n",
    "- There is a specific way to pull down data from NASA\n",
    "- netCDF files are large size and can take much more resourced to compute for data analysis\n",
    "- Convert netCDF to CSV format to reduce the size of data\n",
    "\n",
    "\n",
    "### STEPS: \n",
    "* Ensure that you have the correct credentials to access the data\n",
    "* Setup your environment to work with wget\n",
    "* Use wget to pull down daily files from a subset link provided by NASA \n",
    "* Do some preliminary filtering \n",
    "* Concatenate the data files\n",
    "* final output: master csv file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd3e4835",
   "metadata": {},
   "source": [
    "### Ensure that you have the following completed: \n",
    "* You have registered for an EarthData Login Profile: https://wiki.earthdata.nasa.gov/display/EL/How+To+Register+For+an+EarthData+Login+Profile\n",
    "* You have authorized \"NASA GESDISC DATA ARCHIVE\": https://disc.gsfc.nasa.gov/earthdata-login\n",
    "* Generate the prerequisite files needed for the wget tool: https://disc.gsfc.nasa.gov/information/howto?title=How%20to%20Generate%20Earthdata%20Prerequisite%20Files\n",
    "* Download the wget tool \n",
    "    * This is a little tricky, I was running into authorization issues on the NASA site using the most recent version of wget. I found successs using version 1.19.2\n",
    "    * You can download from: https://eternallybored.org/misc/wget/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f39f5826",
   "metadata": {},
   "source": [
    "## Subset the data as needed\n",
    "\n",
    "At the moment, the dataset that we are using can be found here: https://disc.gsfc.nasa.gov/api/jobs/results/6480ba7f9c692c7cd8c4a794\n",
    "\n",
    "A handy subsetting tool is available\n",
    "\n",
    " ![alt text](Images\\subset_image.jpg)\n",
    "\n",
    " From there, you can subset on a specific region, date range, etc. \n",
    "\n",
    " ![Alt text](Images\\subset_region.jpg)\n",
    "\n",
    " Once you are finished, you will be directed to this screen where you need to download the list of links provided. The list should be as long as the the number of days that you are subsetting from since each link directs to a data file for one day of readings.\n",
    "\n",
    " ![Alt text](Images\\download_links.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54093d0e",
   "metadata": {},
   "source": [
    "## Get the data with wget\n",
    "\n",
    "If you have correctly completed all the previous steps, this should work for you\n",
    "* Open a command prompt on your PC\n",
    "* Set your working directory to the location of your link list as well as the .dodsrc file you created (see the prequisite files mentioned above)\n",
    "* From the command line, run: \n",
    "\n",
    "wget --load-cookies <path of .urs_cookies file> --save-cookies <path of .urs_cookies file> --keep-session-cookies --user=< YOURUSERNAME > --ask-password -P <folder you want to save the data to> --content-disposition -i <link list .txt>\n",
    "\n",
    "For example:\n",
    "\n",
    "wget --load-cookies C:\\Users\\bucky\\ .urs_cookies --save-cookies C:\\Users\\bucky\\ .urs_cookies --keep-session-cookies --user=bbucky --ask-password -P data_2014/ --content-disposition -i 2014_subset_data_links.txt\n",
    "\n",
    "* it takes me roughly 10 seconds to download one file, for reference\n",
    "\n",
    "You should now have your netcdf files downloaded\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8c014d2",
   "metadata": {},
   "source": [
    "## Convert and concatenate multiple netCDF files to master CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a680156",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1652679167658,
     "user": {
      "displayName": "Kikipessa Doll",
      "userId": "16203075636725704864"
     },
     "user_tz": 420
    },
    "id": "9a680156"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import netCDF4 as nc\n",
    "\n",
    "# converting the datetime format\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab238c96",
   "metadata": {
    "id": "ab238c96"
   },
   "source": [
    "## Path to NETCDF files\n",
    "- Locate the downloaded netcdf files directory in pc directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fa1e6c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 214,
     "status": "error",
     "timestamp": 1652679503238,
     "user": {
      "displayName": "Kikipessa Doll",
      "userId": "16203075636725704864"
     },
     "user_tz": 420
    },
    "id": "1fa1e6c8",
    "outputId": "7e95debd-55c7-47b2-e114-2120a08c6304"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\ddrye\\\\OneDrive\\\\Documents\\\\OMSA_Program\\\\OMSA 2023\\\\Summer2023\\\\Practicum\\\\off_git\\\\data\\\\data_netcdf_all\\\\oco2_LtCO2_140906_B11014Ar_230329213249s.SUB.nc4',\n",
       " 'C:\\\\Users\\\\ddrye\\\\OneDrive\\\\Documents\\\\OMSA_Program\\\\OMSA 2023\\\\Summer2023\\\\Practicum\\\\off_git\\\\data\\\\data_netcdf_all\\\\oco2_LtCO2_140907_B11014Ar_230329213320s.SUB.nc4',\n",
       " 'C:\\\\Users\\\\ddrye\\\\OneDrive\\\\Documents\\\\OMSA_Program\\\\OMSA 2023\\\\Summer2023\\\\Practicum\\\\off_git\\\\data\\\\data_netcdf_all\\\\oco2_LtCO2_140908_B11014Ar_230329213350s.SUB.nc4',\n",
       " 'C:\\\\Users\\\\ddrye\\\\OneDrive\\\\Documents\\\\OMSA_Program\\\\OMSA 2023\\\\Summer2023\\\\Practicum\\\\off_git\\\\data\\\\data_netcdf_all\\\\oco2_LtCO2_140909_B11014Ar_230329213426s.SUB.nc4',\n",
       " 'C:\\\\Users\\\\ddrye\\\\OneDrive\\\\Documents\\\\OMSA_Program\\\\OMSA 2023\\\\Summer2023\\\\Practicum\\\\off_git\\\\data\\\\data_netcdf_all\\\\oco2_LtCO2_140910_B11014Ar_230329213451s.SUB.nc4',\n",
       " 'C:\\\\Users\\\\ddrye\\\\OneDrive\\\\Documents\\\\OMSA_Program\\\\OMSA 2023\\\\Summer2023\\\\Practicum\\\\off_git\\\\data\\\\data_netcdf_all\\\\oco2_LtCO2_140911_B11014Ar_230329213526s.SUB.nc4',\n",
       " 'C:\\\\Users\\\\ddrye\\\\OneDrive\\\\Documents\\\\OMSA_Program\\\\OMSA 2023\\\\Summer2023\\\\Practicum\\\\off_git\\\\data\\\\data_netcdf_all\\\\oco2_LtCO2_140912_B11014Ar_230329213557s.SUB.nc4',\n",
       " 'C:\\\\Users\\\\ddrye\\\\OneDrive\\\\Documents\\\\OMSA_Program\\\\OMSA 2023\\\\Summer2023\\\\Practicum\\\\off_git\\\\data\\\\data_netcdf_all\\\\oco2_LtCO2_140913_B11014Ar_230329213633s.SUB.nc4',\n",
       " 'C:\\\\Users\\\\ddrye\\\\OneDrive\\\\Documents\\\\OMSA_Program\\\\OMSA 2023\\\\Summer2023\\\\Practicum\\\\off_git\\\\data\\\\data_netcdf_all\\\\oco2_LtCO2_140914_B11014Ar_230329213705s.SUB.nc4',\n",
       " 'C:\\\\Users\\\\ddrye\\\\OneDrive\\\\Documents\\\\OMSA_Program\\\\OMSA 2023\\\\Summer2023\\\\Practicum\\\\off_git\\\\data\\\\data_netcdf_all\\\\oco2_LtCO2_140915_B11014Ar_230329213733s.SUB.nc4']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_a= (r\"C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_all\")\n",
    "\n",
    "# Collect the paths of each individual files\n",
    "file_names= []\n",
    "\n",
    "for file in os.listdir(path_a):\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".nc4\"):\n",
    "        file_path = f\"{path_a}\\{file}\"\n",
    "      \n",
    "        # Store the path location of each individual files\n",
    "        file_names.append(file_path)\n",
    "        \n",
    "        \n",
    "# check first 10 files path\n",
    "file_names[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd2ad694",
   "metadata": {
    "id": "cd2ad694"
   },
   "source": [
    "# Check the total files in the DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de96547b",
   "metadata": {
    "id": "de96547b",
    "outputId": "2c929c0a-183e-447c-d3cb-2bdb0dba76ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TotalFiles:  2943\n"
     ]
    }
   ],
   "source": [
    "countFiles=0\n",
    "\n",
    "for j in file_names:\n",
    "    if j.endswith(\".nc4\"):\n",
    "        countFiles+=1\n",
    "        #print(j)\n",
    "        \n",
    "print('\\nTotalFiles: ', countFiles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4a27bf7",
   "metadata": {
    "id": "e4a27bf7"
   },
   "source": [
    "# Example: \n",
    "### Opening a single file in netCDF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71c5c0f8",
   "metadata": {
    "id": "71c5c0f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sounding_id_idx',\n",
       " 'longitude',\n",
       " 'latitude',\n",
       " 'time',\n",
       " 'epoch_dimension',\n",
       " 'co2_profile_apriori',\n",
       " 'date',\n",
       " 'file_index',\n",
       " 'pressure_levels',\n",
       " 'pressure_weight',\n",
       " 'sensor_zenith_angle',\n",
       " 'solar_zenith_angle',\n",
       " 'vertex_latitude',\n",
       " 'vertex_longitude',\n",
       " 'xco2',\n",
       " 'xco2_apriori',\n",
       " 'xco2_averaging_kernel',\n",
       " 'xco2_qf_bitflag',\n",
       " 'xco2_quality_flag',\n",
       " 'xco2_uncertainty',\n",
       " 'sounding_id',\n",
       " 'levels',\n",
       " 'vertices']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xco2= nc.Dataset(file_names[9])\n",
    "list(df_xco2.variables.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f1aa9ba",
   "metadata": {
    "id": "8f1aa9ba"
   },
   "source": [
    "# DateTime format Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4133c955",
   "metadata": {
    "id": "4133c955"
   },
   "outputs": [],
   "source": [
    "# DATE time function\n",
    "def conv_date(d):\n",
    "    return datetime.strptime(str(d), '%Y%m%d%H%M%S%f')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7d135ca",
   "metadata": {
    "id": "d7d135ca"
   },
   "source": [
    "### NOTE:\n",
    "- Refine the ENTIRE dataframe by GOOD quality_flag->0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bfa8617",
   "metadata": {
    "id": "5bfa8617"
   },
   "outputs": [],
   "source": [
    "# FUNCTION to convert data\n",
    "\n",
    "def convHdf(path_file, n=0):\n",
    "\n",
    "    data= nc.Dataset(path_file)\n",
    "\n",
    "    #if empty, report and continue\n",
    "    if 'xco2' not in data.variables.keys():\n",
    "        print(path_file,\" is missing xco2 data\")\n",
    "        return \n",
    "\n",
    "    df_xco2= pd.DataFrame()\n",
    "    \n",
    "    #if we want to experiment with different variables\n",
    "    \"\"\"variables=['sounding_id_idx', 'longitude', 'latitude', 'time', 'epoch_dimension', 'co2_profile_apriori', 'date',\n",
    "                'file_index', 'pressure_levels', 'pressure_weight', 'sensor_zenith_angle', 'solar_zenith_angle', 'vertex_latitude',\n",
    "                  'vertex_longitude', 'xco2', 'xco2_apriori', 'xco2_averaging_kernel', 'xco2_qf_bitflag', 'xco2_quality_flag', 'xco2_uncertainty', 'sounding_id']\n",
    "\n",
    "    for i in variables:\n",
    "        print\n",
    "        try:\n",
    "            df_xco2[i]=data.variables[i][:]\n",
    "        except:\n",
    "            pass\"\"\"\n",
    "\n",
    "    df_xco2['Xco2']= data.variables['xco2'][:]\n",
    "    df_xco2['Latitude']= data.variables['latitude'][:]\n",
    "    df_xco2['Longitude']= data.variables['longitude'][:] \n",
    "    df_xco2['xco2_quality_flag']= data.variables['xco2_quality_flag'][:]\n",
    "    \n",
    "    # Date\n",
    "    df_xco2['DateTime']= data.variables['sounding_id'][:]\n",
    "    \n",
    "    #Convert soundingID to datetime format\n",
    "    df_xco2['DateTime']= df_xco2['DateTime'].apply(conv_date)\n",
    "    df_xco2['DateTime']= pd.to_datetime(df_xco2['DateTime'])\n",
    "    \n",
    "    # YEAR and month column\n",
    "    df_xco2['Year']= df_xco2['DateTime'].dt.year\n",
    "    df_xco2['Month']= df_xco2['DateTime'].dt.month\n",
    "    df_xco2['Day']= df_xco2['DateTime'].dt.day\n",
    "    \n",
    "    # Refine the ENTIRE dataframe by GOOD quality_flag->0\n",
    "    # NOTE: REDUCES the size of the file\n",
    "    df_xco2= df_xco2[df_xco2['xco2_quality_flag'] == 0]   \n",
    "    \n",
    "   \n",
    "    date= str(data.variables['sounding_id'][0])      \n",
    "    \n",
    "    return df_xco2\n",
    "    # create a CSV and store on new folder: csv_files\n",
    "    #df_xco2.to_csv('csv_files'+'/'+ data.Sensor+'_xco2_'+ date+'_.txt', index= False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1506ef2f",
   "metadata": {
    "id": "1506ef2f"
   },
   "source": [
    "# Testing: Single file transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6be0ec39",
   "metadata": {
    "id": "6be0ec39"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Xco2</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>xco2_quality_flag</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>402.379364</td>\n",
       "      <td>22.810944</td>\n",
       "      <td>-168.435822</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-07 00:25:53.010</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>402.619751</td>\n",
       "      <td>22.800173</td>\n",
       "      <td>-168.431580</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-07 00:25:53.020</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>402.361694</td>\n",
       "      <td>22.829395</td>\n",
       "      <td>-168.440948</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-07 00:25:53.310</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>402.895264</td>\n",
       "      <td>22.818623</td>\n",
       "      <td>-168.436707</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-07 00:25:53.320</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>402.596832</td>\n",
       "      <td>22.807749</td>\n",
       "      <td>-168.432541</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-07 00:25:53.330</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19447</th>\n",
       "      <td>404.543854</td>\n",
       "      <td>50.542351</td>\n",
       "      <td>-165.457184</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-07 23:38:50.330</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19465</th>\n",
       "      <td>405.779755</td>\n",
       "      <td>50.630581</td>\n",
       "      <td>-165.480438</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-07 23:38:53.070</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19481</th>\n",
       "      <td>405.406738</td>\n",
       "      <td>51.565105</td>\n",
       "      <td>-166.081161</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-07 23:39:10.340</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19483</th>\n",
       "      <td>404.748535</td>\n",
       "      <td>51.569763</td>\n",
       "      <td>-166.076324</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-07 23:39:10.750</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19492</th>\n",
       "      <td>405.790741</td>\n",
       "      <td>52.881878</td>\n",
       "      <td>-166.916565</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-07 23:39:36.770</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14909 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Xco2   Latitude   Longitude  xco2_quality_flag  \\\n",
       "4      402.379364  22.810944 -168.435822                  0   \n",
       "5      402.619751  22.800173 -168.431580                  0   \n",
       "8      402.361694  22.829395 -168.440948                  0   \n",
       "9      402.895264  22.818623 -168.436707                  0   \n",
       "10     402.596832  22.807749 -168.432541                  0   \n",
       "...           ...        ...         ...                ...   \n",
       "19447  404.543854  50.542351 -165.457184                  0   \n",
       "19465  405.779755  50.630581 -165.480438                  0   \n",
       "19481  405.406738  51.565105 -166.081161                  0   \n",
       "19483  404.748535  51.569763 -166.076324                  0   \n",
       "19492  405.790741  52.881878 -166.916565                  0   \n",
       "\n",
       "                     DateTime  Year  Month  Day  \n",
       "4     2017-10-07 00:25:53.010  2017     10    7  \n",
       "5     2017-10-07 00:25:53.020  2017     10    7  \n",
       "8     2017-10-07 00:25:53.310  2017     10    7  \n",
       "9     2017-10-07 00:25:53.320  2017     10    7  \n",
       "10    2017-10-07 00:25:53.330  2017     10    7  \n",
       "...                       ...   ...    ...  ...  \n",
       "19447 2017-10-07 23:38:50.330  2017     10    7  \n",
       "19465 2017-10-07 23:38:53.070  2017     10    7  \n",
       "19481 2017-10-07 23:39:10.340  2017     10    7  \n",
       "19483 2017-10-07 23:39:10.750  2017     10    7  \n",
       "19492 2017-10-07 23:39:36.770  2017     10    7  \n",
       "\n",
       "[14909 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convHdf(file_names[1000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c367997",
   "metadata": {
    "id": "4c367997"
   },
   "source": [
    "## Concatenate all files into one master dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "221c73b6",
   "metadata": {
    "id": "221c73b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_all\\oco2_LtCO2_141023_B11014Ar_230329200159s.SUB.nc4  is missing xco2 data\n",
      "C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_all\\oco2_LtCO2_150129_B11014Ar_230322180219s.SUB.nc4  is missing xco2 data\n",
      "C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_all\\oco2_LtCO2_151026_B11014Ar_230222200809s.SUB.nc4  is missing xco2 data\n",
      "C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_all\\oco2_LtCO2_161229_B11014Ar_230111184801s.SUB.nc4  is missing xco2 data\n",
      "C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_all\\oco2_LtCO2_171024_B11014Ar_221212230141s.SUB.nc4  is missing xco2 data\n",
      "C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_all\\oco2_LtCO2_181110_B11014Ar_221019173153s.SUB.nc4  is missing xco2 data\n",
      "C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_all\\oco2_LtCO2_190304_B11014Ar_221012170715s.SUB.nc4  is missing xco2 data\n",
      "C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_all\\oco2_LtCO2_201230_B11014Ar_220729012754s.SUB.nc4  is missing xco2 data\n",
      "C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_all\\oco2_LtCO2_210201_B11014Ar_220729004203s.SUB.nc4  is missing xco2 data\n",
      "C:\\Users\\ddrye\\OneDrive\\Documents\\OMSA_Program\\OMSA 2023\\Summer2023\\Practicum\\off_git\\data\\data_netcdf_all\\oco2_LtCO2_220506_B11014Ar_220822213813s.SUB.nc4  is missing xco2 data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Xco2</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>xco2_quality_flag</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>389.764069</td>\n",
       "      <td>81.391083</td>\n",
       "      <td>-74.009087</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-06 13:48:24.760</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>388.759888</td>\n",
       "      <td>81.399887</td>\n",
       "      <td>-73.966896</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-06 13:48:24.770</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>389.952484</td>\n",
       "      <td>81.412910</td>\n",
       "      <td>-74.224922</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-06 13:48:25.370</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>392.939789</td>\n",
       "      <td>77.179932</td>\n",
       "      <td>-66.461746</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-06 15:25:20.030</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>389.582825</td>\n",
       "      <td>77.195396</td>\n",
       "      <td>-66.521400</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-06 15:25:20.330</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>391.755737</td>\n",
       "      <td>77.201897</td>\n",
       "      <td>-66.670692</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-06 15:25:20.710</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>390.422668</td>\n",
       "      <td>77.206406</td>\n",
       "      <td>-66.626122</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-06 15:25:20.720</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>391.380463</td>\n",
       "      <td>77.221825</td>\n",
       "      <td>-66.686157</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-06 15:25:21.020</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>390.483276</td>\n",
       "      <td>77.241692</td>\n",
       "      <td>-66.701355</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-06 15:25:21.330</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>393.851318</td>\n",
       "      <td>77.252609</td>\n",
       "      <td>-66.806648</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-06 15:25:21.720</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Xco2   Latitude  Longitude  xco2_quality_flag  \\\n",
       "0  389.764069  81.391083 -74.009087                  0   \n",
       "1  388.759888  81.399887 -73.966896                  0   \n",
       "2  389.952484  81.412910 -74.224922                  0   \n",
       "3  392.939789  77.179932 -66.461746                  0   \n",
       "4  389.582825  77.195396 -66.521400                  0   \n",
       "5  391.755737  77.201897 -66.670692                  0   \n",
       "6  390.422668  77.206406 -66.626122                  0   \n",
       "7  391.380463  77.221825 -66.686157                  0   \n",
       "8  390.483276  77.241692 -66.701355                  0   \n",
       "9  393.851318  77.252609 -66.806648                  0   \n",
       "\n",
       "                 DateTime  Year  Month  Day  \n",
       "0 2014-09-06 13:48:24.760  2014      9    6  \n",
       "1 2014-09-06 13:48:24.770  2014      9    6  \n",
       "2 2014-09-06 13:48:25.370  2014      9    6  \n",
       "3 2014-09-06 15:25:20.030  2014      9    6  \n",
       "4 2014-09-06 15:25:20.330  2014      9    6  \n",
       "5 2014-09-06 15:25:20.710  2014      9    6  \n",
       "6 2014-09-06 15:25:20.720  2014      9    6  \n",
       "7 2014-09-06 15:25:21.020  2014      9    6  \n",
       "8 2014-09-06 15:25:21.330  2014      9    6  \n",
       "9 2014-09-06 15:25:21.720  2014      9    6  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using Function to READ FILES from the direcotry and convert all netCDF files to csv/txt \n",
    "\n",
    "#initializing   \n",
    "df_all = convHdf(file_names[0])\n",
    "\n",
    "for j in range(1, len(file_names)):\n",
    "  \n",
    "       # EG to read FIRST dataset from THE DIRECTORY       \n",
    "        df=convHdf(file_names[j], j)\n",
    "        df_all = pd.concat([df_all,df],ignore_index=True)\n",
    "\n",
    "df_all[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89d824e0",
   "metadata": {},
   "source": [
    "## Write dataframe to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "849effab",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path=r'C:/Users/ddrye/OneDrive/Documents/OMSA_Program/OMSA 2023\\Summer2023/Practicum/off_git/data/'\n",
    "df_all.to_csv(local_path+'data_raw_csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "639678bd",
   "metadata": {},
   "source": [
    "## Checkout breakdown of readings per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "194416c5",
   "metadata": {
    "id": "194416c5"
   },
   "outputs": [],
   "source": [
    "repo_path=\"C:/Users/ddrye/OneDrive/Documents/OMSA_Program/OMSA 2023/Summer2023/Practicum/Team-Project-Practicum-6748/nasa_data/data/\"\n",
    "df_grouped=df_all.groupby(['Year','Month','Day']).size().reset_index().rename(columns={0:'count'})\n",
    "df_grouped.to_csv(repo_path+\"OVERVIEW_data_master\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Conversion_NETCDF_to_CSV_2019_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
